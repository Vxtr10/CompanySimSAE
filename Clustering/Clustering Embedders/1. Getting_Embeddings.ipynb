{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b520465f-b71b-4c3b-a319-d29fbbf76937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/shaox7/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import json\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a95a182-0e9a-4646-bbe2-989b105918d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before: 615.29 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import os\n",
    "process = psutil.Process(os.getpid())\n",
    "print(f\"Memory usage before: {process.memory_info().rss / 1024 ** 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4574092-6791-480e-854d-fbbfb7f22327",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_compinfo = load_dataset(\"Mateusz1017/annual_reports_tokenized_llama3_logged_returns_no_null_returns_and_incomplete_descriptions_24k\")\n",
    "df_compinfo = ds_compinfo['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd970bca-3513-4fac-846d-507098d787fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compinfo = df_compinfo.dropna(subset=['sic_code'])\n",
    "df_compinfo = df_compinfo.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb7bdc6-862b-4a04-a795-2977b436dba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>year</th>\n",
       "      <th>section_1</th>\n",
       "      <th>company_name</th>\n",
       "      <th>sic_code</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>ticker</th>\n",
       "      <th>returns</th>\n",
       "      <th>logged_monthly_returns_matrix</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>input_ids_length</th>\n",
       "      <th>industry_classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75252</td>\n",
       "      <td>1993</td>\n",
       "      <td>Item 1. Business\\nOwens &amp; Minor, Inc. (the \"Co...</td>\n",
       "      <td>OWENS &amp; MINOR INC/VA/</td>\n",
       "      <td>5047</td>\n",
       "      <td>[128000, 1256, 220, 16, 13, 8184, 198, 46, 86,...</td>\n",
       "      <td>[OMI]</td>\n",
       "      <td>0.569779</td>\n",
       "      <td>[-0.06939241136032089, 0.07496338743638563, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>2558</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40533</td>\n",
       "      <td>1993</td>\n",
       "      <td>ITEM 1. BUSINESS\\nINTRODUCTION\\nGeneral Dynami...</td>\n",
       "      <td>GENERAL DYNAMICS CORP</td>\n",
       "      <td>3730</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 198, 3301,...</td>\n",
       "      <td>[GD]</td>\n",
       "      <td>0.272757</td>\n",
       "      <td>[0.03622371368058769, -0.181048534708107, -0.0...</td>\n",
       "      <td>3</td>\n",
       "      <td>3445</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91576</td>\n",
       "      <td>1993</td>\n",
       "      <td>ITEM 1. BUSINESS\\nOVERVIEW\\nOn March 1, 1994, ...</td>\n",
       "      <td>KEYCORP /NEW/</td>\n",
       "      <td>6021</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 198, 50205...</td>\n",
       "      <td>[KEY, KEY-PJ, KEY-PK, KEY-PI, KEY-PL]</td>\n",
       "      <td>0.029588</td>\n",
       "      <td>[0.03969967097211506, 0.06939185891396345, -0....</td>\n",
       "      <td>5</td>\n",
       "      <td>6640</td>\n",
       "      <td>Finance, Insurance, And Real Estate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7536</td>\n",
       "      <td>1993</td>\n",
       "      <td>Item 1. Business.\\nArrow Electronics, Inc. (th...</td>\n",
       "      <td>ARROW ELECTRONICS INC</td>\n",
       "      <td>5065</td>\n",
       "      <td>[128000, 1256, 220, 16, 13, 8184, 627, 27003, ...</td>\n",
       "      <td>[ARW]</td>\n",
       "      <td>0.518182</td>\n",
       "      <td>[0.004008021397538868, 0.01587334915629016, 0....</td>\n",
       "      <td>6</td>\n",
       "      <td>2464</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10456</td>\n",
       "      <td>1993</td>\n",
       "      <td>ITEM 1. BUSINESS.\\n(a) GENERAL DEVELOPMENT OF ...</td>\n",
       "      <td>BAXTER INTERNATIONAL INC</td>\n",
       "      <td>3841</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 627, 2948,...</td>\n",
       "      <td>[BAX]</td>\n",
       "      <td>-0.206450</td>\n",
       "      <td>[-0.04613021404865702, 0.029600784670023767, -...</td>\n",
       "      <td>7</td>\n",
       "      <td>4006</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27717</th>\n",
       "      <td>74046</td>\n",
       "      <td>2019</td>\n",
       "      <td>ITEM 1 - BUSINESS\\nOVERVIEW OF BUSINESS\\nIn 19...</td>\n",
       "      <td>Oil-Dri Corp of America</td>\n",
       "      <td>3990</td>\n",
       "      <td>[128000, 12236, 220, 16, 482, 27693, 198, 5020...</td>\n",
       "      <td>[ODC]</td>\n",
       "      <td>0.360402</td>\n",
       "      <td>[0.09380163057314017, 0.07467147786746844, 0.0...</td>\n",
       "      <td>61772</td>\n",
       "      <td>2419</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27718</th>\n",
       "      <td>1750</td>\n",
       "      <td>2019</td>\n",
       "      <td>ITEM 1. BUSINESS\\nGeneral\\nAAR CORP. and its s...</td>\n",
       "      <td>AAR CORP</td>\n",
       "      <td>3720</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 198, 15777...</td>\n",
       "      <td>[AIR]</td>\n",
       "      <td>0.213843</td>\n",
       "      <td>[-0.028972527271497482, -0.11658608430267486, ...</td>\n",
       "      <td>61782</td>\n",
       "      <td>3345</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27719</th>\n",
       "      <td>80420</td>\n",
       "      <td>2019</td>\n",
       "      <td>Item 1. Business\\nOverview\\nPowell Industries,...</td>\n",
       "      <td>POWELL INDUSTRIES INC</td>\n",
       "      <td>3613</td>\n",
       "      <td>[128000, 1256, 220, 16, 13, 8184, 198, 42144, ...</td>\n",
       "      <td>[POWL]</td>\n",
       "      <td>0.883976</td>\n",
       "      <td>[0.13955951729443583, -0.1815976884770055, 0.0...</td>\n",
       "      <td>61785</td>\n",
       "      <td>2292</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27720</th>\n",
       "      <td>934796</td>\n",
       "      <td>2019</td>\n",
       "      <td>ITEM 1.BUSINESS\\nOverview of Our Business\\nOur...</td>\n",
       "      <td>NETWORK CN INC</td>\n",
       "      <td>7310</td>\n",
       "      <td>[128000, 12236, 220, 16, 1823, 2078, 24221, 19...</td>\n",
       "      <td>[NWCN]</td>\n",
       "      <td>-0.775000</td>\n",
       "      <td>[-0.3930425704489719, 0.0, 0.0, 0.287682028300...</td>\n",
       "      <td>61786</td>\n",
       "      <td>4744</td>\n",
       "      <td>Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27721</th>\n",
       "      <td>350868</td>\n",
       "      <td>2019</td>\n",
       "      <td>ITEM 1. BUSINESS\\nOverview\\nIteris, Inc. (refe...</td>\n",
       "      <td>ITERIS, INC.</td>\n",
       "      <td>3669</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 198, 42144...</td>\n",
       "      <td>[ITI]</td>\n",
       "      <td>0.367123</td>\n",
       "      <td>[0.11518231846351687, -0.035339387885827966, 0...</td>\n",
       "      <td>61787</td>\n",
       "      <td>5206</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27722 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik  year                                          section_1  \\\n",
       "0       75252  1993  Item 1. Business\\nOwens & Minor, Inc. (the \"Co...   \n",
       "1       40533  1993  ITEM 1. BUSINESS\\nINTRODUCTION\\nGeneral Dynami...   \n",
       "2       91576  1993  ITEM 1. BUSINESS\\nOVERVIEW\\nOn March 1, 1994, ...   \n",
       "3        7536  1993  Item 1. Business.\\nArrow Electronics, Inc. (th...   \n",
       "4       10456  1993  ITEM 1. BUSINESS.\\n(a) GENERAL DEVELOPMENT OF ...   \n",
       "...       ...   ...                                                ...   \n",
       "27717   74046  2019  ITEM 1 - BUSINESS\\nOVERVIEW OF BUSINESS\\nIn 19...   \n",
       "27718    1750  2019  ITEM 1. BUSINESS\\nGeneral\\nAAR CORP. and its s...   \n",
       "27719   80420  2019  Item 1. Business\\nOverview\\nPowell Industries,...   \n",
       "27720  934796  2019  ITEM 1.BUSINESS\\nOverview of Our Business\\nOur...   \n",
       "27721  350868  2019  ITEM 1. BUSINESS\\nOverview\\nIteris, Inc. (refe...   \n",
       "\n",
       "                   company_name sic_code  \\\n",
       "0         OWENS & MINOR INC/VA/     5047   \n",
       "1         GENERAL DYNAMICS CORP     3730   \n",
       "2                 KEYCORP /NEW/     6021   \n",
       "3         ARROW ELECTRONICS INC     5065   \n",
       "4      BAXTER INTERNATIONAL INC     3841   \n",
       "...                         ...      ...   \n",
       "27717   Oil-Dri Corp of America     3990   \n",
       "27718                  AAR CORP     3720   \n",
       "27719     POWELL INDUSTRIES INC     3613   \n",
       "27720            NETWORK CN INC     7310   \n",
       "27721              ITERIS, INC.     3669   \n",
       "\n",
       "                                               input_ids  \\\n",
       "0      [128000, 1256, 220, 16, 13, 8184, 198, 46, 86,...   \n",
       "1      [128000, 12236, 220, 16, 13, 27693, 198, 3301,...   \n",
       "2      [128000, 12236, 220, 16, 13, 27693, 198, 50205...   \n",
       "3      [128000, 1256, 220, 16, 13, 8184, 627, 27003, ...   \n",
       "4      [128000, 12236, 220, 16, 13, 27693, 627, 2948,...   \n",
       "...                                                  ...   \n",
       "27717  [128000, 12236, 220, 16, 482, 27693, 198, 5020...   \n",
       "27718  [128000, 12236, 220, 16, 13, 27693, 198, 15777...   \n",
       "27719  [128000, 1256, 220, 16, 13, 8184, 198, 42144, ...   \n",
       "27720  [128000, 12236, 220, 16, 1823, 2078, 24221, 19...   \n",
       "27721  [128000, 12236, 220, 16, 13, 27693, 198, 42144...   \n",
       "\n",
       "                                      ticker   returns  \\\n",
       "0                                      [OMI]  0.569779   \n",
       "1                                       [GD]  0.272757   \n",
       "2      [KEY, KEY-PJ, KEY-PK, KEY-PI, KEY-PL]  0.029588   \n",
       "3                                      [ARW]  0.518182   \n",
       "4                                      [BAX] -0.206450   \n",
       "...                                      ...       ...   \n",
       "27717                                  [ODC]  0.360402   \n",
       "27718                                  [AIR]  0.213843   \n",
       "27719                                 [POWL]  0.883976   \n",
       "27720                                 [NWCN] -0.775000   \n",
       "27721                                  [ITI]  0.367123   \n",
       "\n",
       "                           logged_monthly_returns_matrix  __index_level_0__  \\\n",
       "0      [-0.06939241136032089, 0.07496338743638563, 0....                  0   \n",
       "1      [0.03622371368058769, -0.181048534708107, -0.0...                  3   \n",
       "2      [0.03969967097211506, 0.06939185891396345, -0....                  5   \n",
       "3      [0.004008021397538868, 0.01587334915629016, 0....                  6   \n",
       "4      [-0.04613021404865702, 0.029600784670023767, -...                  7   \n",
       "...                                                  ...                ...   \n",
       "27717  [0.09380163057314017, 0.07467147786746844, 0.0...              61772   \n",
       "27718  [-0.028972527271497482, -0.11658608430267486, ...              61782   \n",
       "27719  [0.13955951729443583, -0.1815976884770055, 0.0...              61785   \n",
       "27720  [-0.3930425704489719, 0.0, 0.0, 0.287682028300...              61786   \n",
       "27721  [0.11518231846351687, -0.035339387885827966, 0...              61787   \n",
       "\n",
       "       input_ids_length              industry_classification  \n",
       "0                  2558                      Wholesale Trade  \n",
       "1                  3445                        Manufacturing  \n",
       "2                  6640  Finance, Insurance, And Real Estate  \n",
       "3                  2464                      Wholesale Trade  \n",
       "4                  4006                        Manufacturing  \n",
       "...                 ...                                  ...  \n",
       "27717              2419                        Manufacturing  \n",
       "27718              3345                        Manufacturing  \n",
       "27719              2292                        Manufacturing  \n",
       "27720              4744                             Services  \n",
       "27721              5206                        Manufacturing  \n",
       "\n",
       "[27722 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to classify SIC codes into industries based on the first two digits\n",
    "def classify_sic(sic_code):\n",
    "    # Extract the first two digits of the SIC code\n",
    "    first_two_digits = int(str(sic_code)[:2])\n",
    "    \n",
    "    # Map to industry categories\n",
    "    if 1 <= first_two_digits <= 9:\n",
    "        return 'Agriculture, Forestry, And Fishing'\n",
    "    elif 10 <= first_two_digits <= 14:\n",
    "        return 'Mining'\n",
    "    elif 15 <= first_two_digits <= 17:\n",
    "        return 'Construction'\n",
    "    elif 20 <= first_two_digits <= 39:\n",
    "        return 'Manufacturing'\n",
    "    elif 40 <= first_two_digits <= 49:\n",
    "        return 'Transportation, Communications, Electric, Gas, And Sanitary Services'\n",
    "    elif 50 <= first_two_digits <= 51:\n",
    "        return 'Wholesale Trade'\n",
    "    elif 52 <= first_two_digits <= 59:\n",
    "        return 'Retail Trade'\n",
    "    elif 60 <= first_two_digits <= 67:\n",
    "        return 'Finance, Insurance, And Real Estate'\n",
    "    elif 70 <= first_two_digits <= 89:\n",
    "        return 'Services'\n",
    "    elif 90 <= first_two_digits <= 99:\n",
    "        return 'Public Administration'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply the classification to the SIC codes in the dataset\n",
    "df_compinfo['industry_classification'] = df_compinfo['sic_code'].apply(classify_sic)\n",
    "df_compinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0434fd46-62d1-4b81-a932-1ba3a6cb1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_compinfo = df_compinfo[[\"cik\", \"year\", \"sic_code\", \"ticker\", \"__index_level_0__\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80b407ea-a6dd-4df9-8cfb-bf4e7b1945ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27722 entries, 0 to 27721\n",
      "Data columns (total 12 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   cik                            27722 non-null  object \n",
      " 1   year                           27722 non-null  object \n",
      " 2   section_1                      27722 non-null  object \n",
      " 3   company_name                   27722 non-null  object \n",
      " 4   sic_code                       27722 non-null  object \n",
      " 5   input_ids                      27722 non-null  object \n",
      " 6   ticker                         27722 non-null  object \n",
      " 7   returns                        27709 non-null  float64\n",
      " 8   logged_monthly_returns_matrix  27722 non-null  object \n",
      " 9   __index_level_0__              27722 non-null  int64  \n",
      " 10  input_ids_length               27722 non-null  int64  \n",
      " 11  industry_classification        27722 non-null  object \n",
      "dtypes: float64(1), int64(2), object(9)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_compinfo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87de96a6-ff32-4d8d-bd7b-34aa65a4caa1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating BERT Embeddings: 100%|██████████| 27722/27722 [47:46<00:00,  9.67it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to get embeddings from BERT\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize the text\n",
    "    tokens = bert_tokenizer(text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():  # No gradients needed for inference\n",
    "        outputs = bert_model(**tokens)\n",
    "        # outputs.last_hidden_state: [batch_size, seq_length, hidden_size]\n",
    "        \n",
    "        # Use the CLS token embedding (first token)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Alternatively, use the mean of all token embeddings\n",
    "        # mean_embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    return cls_embedding.squeeze().tolist()  # Return as a Python list\n",
    "\n",
    "# Apply the embedding function to the DataFrame\n",
    "tqdm.pandas(desc=\"Generating BERT Embeddings\")\n",
    "df_compinfo[\"BERT-embedding\"] = df_compinfo[\"section_1\"].progress_apply(get_bert_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e43df452-ebbe-4845-843e-8dc189b085c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "160d0c24-eba3-4916-a818-6259864648ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_compinfo.to_pickle(\"BERT-embedded_csv.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77dd76ee-9457-45bf-ab95-93847e03319d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating SBERT Embeddings: 100%|██████████| 27722/27722 [13:12<00:00, 34.96it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load SBERT model\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Function to get SBERT embeddings\n",
    "def get_sbert_embedding(text):\n",
    "    return sbert_model.encode(text, convert_to_tensor=True).tolist()\n",
    "\n",
    "# Apply the embedding function to the DataFrame\n",
    "tqdm.pandas(desc=\"Generating SBERT Embeddings\")\n",
    "df_compinfo[\"SBERT-embedding\"] = df_compinfo[\"section_1\"].progress_apply(get_sbert_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fec1114-8a09-4b91-9418-7e05686b5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load SBERT model\n",
    "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d1e38-735d-4aac-b8ab-84811ff23a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aace1453-8772-4eac-838b-832f9070010f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model.max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b8ab41-a903-424d-b47f-0bc9ddad0ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_model[0].auto_model.config.max_position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b54edc4-5dbb-40f8-82a7-b95c04d906a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compinfo[\"year\"] = df_compinfo[\"year\"].astype(int)\n",
    "df_compinfo = df_compinfo[~df_compinfo['year'].isin([1993, 1994, 1995])]\n",
    "df_compinfo.reset_index(drop=True)\n",
    "df_compinfo = df_compinfo.sort_values(by=['year'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cf070b9-4a5e-4075-85c7-eddf56f75d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>year</th>\n",
       "      <th>section_1</th>\n",
       "      <th>company_name</th>\n",
       "      <th>sic_code</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>ticker</th>\n",
       "      <th>returns</th>\n",
       "      <th>logged_monthly_returns_matrix</th>\n",
       "      <th>__index_level_0__</th>\n",
       "      <th>input_ids_length</th>\n",
       "      <th>industry_classification</th>\n",
       "      <th>BERT-embedding</th>\n",
       "      <th>SBERT-embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>862861</td>\n",
       "      <td>1996</td>\n",
       "      <td>ITEM 1. BUSINESS\\nGENERAL\\nAppliance Recycling...</td>\n",
       "      <td>JanOne Inc.</td>\n",
       "      <td>5700</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 198, 95836...</td>\n",
       "      <td>[ALTS]</td>\n",
       "      <td>-0.864865</td>\n",
       "      <td>[-0.2876819162165623, 0.3184534375856794, 0.08...</td>\n",
       "      <td>1538</td>\n",
       "      <td>4465</td>\n",
       "      <td>Retail Trade</td>\n",
       "      <td>[-1.0645477771759033, 0.23505006730556488, -0....</td>\n",
       "      <td>[-0.07066744565963745, -0.03502631559967995, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>109380</td>\n",
       "      <td>1996</td>\n",
       "      <td>ITEM 1. BUSINESS\\nZions Bancorporation (the Pa...</td>\n",
       "      <td>ZIONS BANCORPORATION, NATIONAL ASSOCIATION /UT/</td>\n",
       "      <td>6021</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 198, 57, 9...</td>\n",
       "      <td>[ZION, ZIONP, ZIONL, ZIONO]</td>\n",
       "      <td>0.333699</td>\n",
       "      <td>[-0.036610412591151206, -0.041527657379601665,...</td>\n",
       "      <td>1563</td>\n",
       "      <td>7232</td>\n",
       "      <td>Finance, Insurance, And Real Estate</td>\n",
       "      <td>[-0.6020901203155518, -0.01382492296397686, -0...</td>\n",
       "      <td>[0.055379994213581085, -0.0656338632106781, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>102037</td>\n",
       "      <td>1996</td>\n",
       "      <td>ITEM 1. BUSINESS\\nA. The Company\\nUniversal Co...</td>\n",
       "      <td>UNIVERSAL CORP /VA/</td>\n",
       "      <td>5150</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 198, 32, 1...</td>\n",
       "      <td>[UVV]</td>\n",
       "      <td>0.379314</td>\n",
       "      <td>[0.15015591718026108, -0.06733310337040097, -0...</td>\n",
       "      <td>1547</td>\n",
       "      <td>4012</td>\n",
       "      <td>Wholesale Trade</td>\n",
       "      <td>[-0.8490127921104431, -0.3367622494697571, -0....</td>\n",
       "      <td>[0.024468760937452316, -0.03050614334642887, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>710752</td>\n",
       "      <td>1996</td>\n",
       "      <td>ITEM 1. BUSINESS.\\nDESCRIPTION OF THE TRUST\\nS...</td>\n",
       "      <td>SABINE ROYALTY TRUST</td>\n",
       "      <td>6792</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 627, 46533...</td>\n",
       "      <td>[SBR]</td>\n",
       "      <td>0.771184</td>\n",
       "      <td>[0.01415349005774118, 0.08221469119141876, 0.1...</td>\n",
       "      <td>1561</td>\n",
       "      <td>7794</td>\n",
       "      <td>Finance, Insurance, And Real Estate</td>\n",
       "      <td>[-0.5184245109558105, -0.05737003684043884, -0...</td>\n",
       "      <td>[-0.04296134039759636, -0.03728189691901207, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>72333</td>\n",
       "      <td>1996</td>\n",
       "      <td>Item 1. Business. - ------------------\\nNordst...</td>\n",
       "      <td>NORDSTROM INC</td>\n",
       "      <td>5651</td>\n",
       "      <td>[128000, 1256, 220, 16, 13, 8184, 13, 482, 147...</td>\n",
       "      <td>[JWN]</td>\n",
       "      <td>-0.118097</td>\n",
       "      <td>[0.13948494150378526, 0.07371579519008523, 0.0...</td>\n",
       "      <td>1559</td>\n",
       "      <td>566</td>\n",
       "      <td>Retail Trade</td>\n",
       "      <td>[-0.7209739089012146, 0.13205184042453766, -0....</td>\n",
       "      <td>[0.04498894512653351, -0.01455921120941639, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3676</th>\n",
       "      <td>1310630</td>\n",
       "      <td>2020</td>\n",
       "      <td>ITEM 1. BUSINESS\\nChina Foods Holdings Ltd. (t...</td>\n",
       "      <td>China Foods Holdings Ltd.</td>\n",
       "      <td>2833</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 198, 23078...</td>\n",
       "      <td>[CFOO]</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.08434115895221751,...</td>\n",
       "      <td>6865</td>\n",
       "      <td>1750</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>[-0.5753610134124756, -0.1743033528327942, -0....</td>\n",
       "      <td>[-0.037535540759563446, -0.05656890198588371, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>798783</td>\n",
       "      <td>2020</td>\n",
       "      <td>ITEM 1.\\nBusiness\\nGeneral\\nWe are a real esta...</td>\n",
       "      <td>UNIVERSAL HEALTH REALTY INCOME TRUST</td>\n",
       "      <td>6798</td>\n",
       "      <td>[128000, 12236, 220, 16, 627, 23562, 198, 1577...</td>\n",
       "      <td>[UHT]</td>\n",
       "      <td>-0.418372</td>\n",
       "      <td>[-0.13549047531544484, -0.06629779212256275, 0...</td>\n",
       "      <td>6830</td>\n",
       "      <td>7235</td>\n",
       "      <td>Finance, Insurance, And Real Estate</td>\n",
       "      <td>[-0.6004636883735657, -0.09270477294921875, 0....</td>\n",
       "      <td>[-0.015440104529261589, -0.05609987676143646, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3646</th>\n",
       "      <td>914122</td>\n",
       "      <td>2020</td>\n",
       "      <td>Item 1. BUSINESS\\nPerma-Pipe International Hol...</td>\n",
       "      <td>Perma-Pipe International Holdings, Inc.</td>\n",
       "      <td>3564</td>\n",
       "      <td>[128000, 1256, 220, 16, 13, 27693, 198, 3976, ...</td>\n",
       "      <td>[PPIH]</td>\n",
       "      <td>-0.339479</td>\n",
       "      <td>[-0.033787020441788815, -0.3389954443959665, -...</td>\n",
       "      <td>6784</td>\n",
       "      <td>2440</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>[-0.48827219009399414, -0.09819140285253525, -...</td>\n",
       "      <td>[-0.049300942569971085, -0.0038823760114610195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>887596</td>\n",
       "      <td>2020</td>\n",
       "      <td>ITEM 1. BUSINESS\\nGeneral\\nThe Cheesecake Fact...</td>\n",
       "      <td>CHEESECAKE FACTORY INC</td>\n",
       "      <td>5812</td>\n",
       "      <td>[128000, 12236, 220, 16, 13, 27693, 198, 15777...</td>\n",
       "      <td>[CAKE]</td>\n",
       "      <td>-0.038025</td>\n",
       "      <td>[-0.07486943363771757, -0.7352798500683871, 0....</td>\n",
       "      <td>6992</td>\n",
       "      <td>613</td>\n",
       "      <td>Retail Trade</td>\n",
       "      <td>[-0.5994244813919067, -0.214384064078331, -0.0...</td>\n",
       "      <td>[0.03250890225172043, -0.10284492373466492, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>1318268</td>\n",
       "      <td>2020</td>\n",
       "      <td>Item 1. Business.\\nSummary\\nMadison Technologi...</td>\n",
       "      <td>Madison Technologies Inc.</td>\n",
       "      <td>5900</td>\n",
       "      <td>[128000, 1256, 220, 16, 13, 8184, 627, 19791, ...</td>\n",
       "      <td>[MDEX]</td>\n",
       "      <td>4.710000</td>\n",
       "      <td>[0.01980259369552205, 0.16251892584552946, -0....</td>\n",
       "      <td>7002</td>\n",
       "      <td>3475</td>\n",
       "      <td>Retail Trade</td>\n",
       "      <td>[-0.6210410594940186, -0.05042947083711624, -0...</td>\n",
       "      <td>[-0.10414830595254898, -0.038725417107343674, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26769 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cik  year                                          section_1  \\\n",
       "975    862861  1996  ITEM 1. BUSINESS\\nGENERAL\\nAppliance Recycling...   \n",
       "992    109380  1996  ITEM 1. BUSINESS\\nZions Bancorporation (the Pa...   \n",
       "982    102037  1996  ITEM 1. BUSINESS\\nA. The Company\\nUniversal Co...   \n",
       "990    710752  1996  ITEM 1. BUSINESS.\\nDESCRIPTION OF THE TRUST\\nS...   \n",
       "989     72333  1996  Item 1. Business. - ------------------\\nNordst...   \n",
       "...       ...   ...                                                ...   \n",
       "3676  1310630  2020  ITEM 1. BUSINESS\\nChina Foods Holdings Ltd. (t...   \n",
       "3661   798783  2020  ITEM 1.\\nBusiness\\nGeneral\\nWe are a real esta...   \n",
       "3646   914122  2020  Item 1. BUSINESS\\nPerma-Pipe International Hol...   \n",
       "3727   887596  2020  ITEM 1. BUSINESS\\nGeneral\\nThe Cheesecake Fact...   \n",
       "3732  1318268  2020  Item 1. Business.\\nSummary\\nMadison Technologi...   \n",
       "\n",
       "                                         company_name sic_code  \\\n",
       "975                                       JanOne Inc.     5700   \n",
       "992   ZIONS BANCORPORATION, NATIONAL ASSOCIATION /UT/     6021   \n",
       "982                               UNIVERSAL CORP /VA/     5150   \n",
       "990                              SABINE ROYALTY TRUST     6792   \n",
       "989                                     NORDSTROM INC     5651   \n",
       "...                                               ...      ...   \n",
       "3676                        China Foods Holdings Ltd.     2833   \n",
       "3661             UNIVERSAL HEALTH REALTY INCOME TRUST     6798   \n",
       "3646          Perma-Pipe International Holdings, Inc.     3564   \n",
       "3727                           CHEESECAKE FACTORY INC     5812   \n",
       "3732                        Madison Technologies Inc.     5900   \n",
       "\n",
       "                                              input_ids  \\\n",
       "975   [128000, 12236, 220, 16, 13, 27693, 198, 95836...   \n",
       "992   [128000, 12236, 220, 16, 13, 27693, 198, 57, 9...   \n",
       "982   [128000, 12236, 220, 16, 13, 27693, 198, 32, 1...   \n",
       "990   [128000, 12236, 220, 16, 13, 27693, 627, 46533...   \n",
       "989   [128000, 1256, 220, 16, 13, 8184, 13, 482, 147...   \n",
       "...                                                 ...   \n",
       "3676  [128000, 12236, 220, 16, 13, 27693, 198, 23078...   \n",
       "3661  [128000, 12236, 220, 16, 627, 23562, 198, 1577...   \n",
       "3646  [128000, 1256, 220, 16, 13, 27693, 198, 3976, ...   \n",
       "3727  [128000, 12236, 220, 16, 13, 27693, 198, 15777...   \n",
       "3732  [128000, 1256, 220, 16, 13, 8184, 627, 19791, ...   \n",
       "\n",
       "                           ticker   returns  \\\n",
       "975                        [ALTS] -0.864865   \n",
       "992   [ZION, ZIONP, ZIONL, ZIONO]  0.333699   \n",
       "982                         [UVV]  0.379314   \n",
       "990                         [SBR]  0.771184   \n",
       "989                         [JWN] -0.118097   \n",
       "...                           ...       ...   \n",
       "3676                       [CFOO]  0.088000   \n",
       "3661                        [UHT] -0.418372   \n",
       "3646                       [PPIH] -0.339479   \n",
       "3727                       [CAKE] -0.038025   \n",
       "3732                       [MDEX]  4.710000   \n",
       "\n",
       "                          logged_monthly_returns_matrix  __index_level_0__  \\\n",
       "975   [-0.2876819162165623, 0.3184534375856794, 0.08...               1538   \n",
       "992   [-0.036610412591151206, -0.041527657379601665,...               1563   \n",
       "982   [0.15015591718026108, -0.06733310337040097, -0...               1547   \n",
       "990   [0.01415349005774118, 0.08221469119141876, 0.1...               1561   \n",
       "989   [0.13948494150378526, 0.07371579519008523, 0.0...               1559   \n",
       "...                                                 ...                ...   \n",
       "3676  [0.0, 0.0, 0.0, 0.0, 0.0, 0.08434115895221751,...               6865   \n",
       "3661  [-0.13549047531544484, -0.06629779212256275, 0...               6830   \n",
       "3646  [-0.033787020441788815, -0.3389954443959665, -...               6784   \n",
       "3727  [-0.07486943363771757, -0.7352798500683871, 0....               6992   \n",
       "3732  [0.01980259369552205, 0.16251892584552946, -0....               7002   \n",
       "\n",
       "      input_ids_length              industry_classification  \\\n",
       "975               4465                         Retail Trade   \n",
       "992               7232  Finance, Insurance, And Real Estate   \n",
       "982               4012                      Wholesale Trade   \n",
       "990               7794  Finance, Insurance, And Real Estate   \n",
       "989                566                         Retail Trade   \n",
       "...                ...                                  ...   \n",
       "3676              1750                        Manufacturing   \n",
       "3661              7235  Finance, Insurance, And Real Estate   \n",
       "3646              2440                        Manufacturing   \n",
       "3727               613                         Retail Trade   \n",
       "3732              3475                         Retail Trade   \n",
       "\n",
       "                                         BERT-embedding  \\\n",
       "975   [-1.0645477771759033, 0.23505006730556488, -0....   \n",
       "992   [-0.6020901203155518, -0.01382492296397686, -0...   \n",
       "982   [-0.8490127921104431, -0.3367622494697571, -0....   \n",
       "990   [-0.5184245109558105, -0.05737003684043884, -0...   \n",
       "989   [-0.7209739089012146, 0.13205184042453766, -0....   \n",
       "...                                                 ...   \n",
       "3676  [-0.5753610134124756, -0.1743033528327942, -0....   \n",
       "3661  [-0.6004636883735657, -0.09270477294921875, 0....   \n",
       "3646  [-0.48827219009399414, -0.09819140285253525, -...   \n",
       "3727  [-0.5994244813919067, -0.214384064078331, -0.0...   \n",
       "3732  [-0.6210410594940186, -0.05042947083711624, -0...   \n",
       "\n",
       "                                        SBERT-embedding  \n",
       "975   [-0.07066744565963745, -0.03502631559967995, 0...  \n",
       "992   [0.055379994213581085, -0.0656338632106781, -0...  \n",
       "982   [0.024468760937452316, -0.03050614334642887, -...  \n",
       "990   [-0.04296134039759636, -0.03728189691901207, 0...  \n",
       "989   [0.04498894512653351, -0.01455921120941639, -0...  \n",
       "...                                                 ...  \n",
       "3676  [-0.037535540759563446, -0.05656890198588371, ...  \n",
       "3661  [-0.015440104529261589, -0.05609987676143646, ...  \n",
       "3646  [-0.049300942569971085, -0.0038823760114610195...  \n",
       "3727  [0.03250890225172043, -0.10284492373466492, 0....  \n",
       "3732  [-0.10414830595254898, -0.038725417107343674, ...  \n",
       "\n",
       "[26769 rows x 14 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa3ebdf6-d53c-483d-82b0-269b7db86402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_compinfo.to_pickle(\"Embedded_part2.pkl\")\n",
    "df_compinfo = pd.read_pickle(\"Embedded_part2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d294dc2-3d46-4690-bf67-eae335e6bd88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compinfo[\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4afd7932-c403-4eb9-892f-afaaca319006",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['cik', 'year', 'section_1', 'company_name', 'sic_code', 'input_ids', 'ticker', 'returns', 'logged_monthly_returns_matrix', '__index_level_0__', 'input_ids_length', 'industry_classification', 'BERT-embedding', 'SBERT-embedding'],\n",
       "    num_rows: 26769\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Optionally, convert to a Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "final_dataset = Dataset.from_pandas(df_compinfo, preserve_index=False)\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec04217-cc9e-4c20-93b8-7e99f6d2d806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84486bd-f5b0-48c1-b507-64d5e44b9cc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /users/shaox7/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "ploading the dataset shards: 100%|██████████| 6/6 [00:38<00:00,  6.48s/it]ba/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/v1ctor10/BERT_SBERT_embeddings_SAE/commit/b99e35296729878ee4b1242692ef94180badf174', commit_message='Upload dataset', commit_description='', oid='b99e35296729878ee4b1242692ef94180badf174', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# login(token=\"\") \n",
    "# final_dataset.push_to_hub(\"v1ctor10/BERT_SBERT_embeddings_SAE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "616ba233-802a-4bd0-a980-3db790940752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(df_compinfo[\"SBERT-embedding\"].iloc[3]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565232b1-9591-4be8-848c-1ffc59a30055",
   "metadata": {},
   "source": [
    "# To 1536 tokens (as per blackrock paper https://arxiv.org/pdf/2308.08031)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "486e3ad1-d9aa-4905-99ec-93fccb96d614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2558 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "enerating BERT Embeddings: 100%|██████████| 27722/27722 [2:10:08<00:00,  3.55it/s]  "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ====================\n",
    "# 1. Configuration\n",
    "# ====================\n",
    "\n",
    "# Define constants\n",
    "MAX_TOTAL_TOKENS = 1536       # Maximum total tokens per document\n",
    "CHUNK_SIZE = 512              # Maximum tokens per chunk\n",
    "MODEL_NAME = \"bert-base-uncased\"  # Pre-trained BERT model\n",
    "\n",
    "# ====================\n",
    "# 2. Setup Device\n",
    "# ====================\n",
    "\n",
    "# Utilize GPU if available for faster computations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====================\n",
    "# 3. Load Tokenizer and Model\n",
    "# ====================\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "bert_model.to(device)\n",
    "bert_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# ====================\n",
    "# 4. Define Helper Functions\n",
    "# ====================\n",
    "\n",
    "def split_text_into_chunks(text, tokenizer, max_length=CHUNK_SIZE, max_total_tokens=MAX_TOTAL_TOKENS):\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks of tokens with a maximum length, ensuring total tokens do not exceed max_total_tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input document as a string.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use.\n",
    "        max_length (int): Maximum number of tokens per chunk.\n",
    "        max_total_tokens (int): Maximum total tokens per document.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Tokenize the entire text without adding special tokens\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    # Limit to the first max_total_tokens\n",
    "    tokens = tokens[:max_total_tokens]\n",
    "    \n",
    "    # Split tokens into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk_tokens = tokens[i:i + max_length]\n",
    "        # Decode tokens back to string\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_bert_embedding_chunk(chunk_text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Generates the embedding for a single text chunk using BERT.\n",
    "\n",
    "    Args:\n",
    "        chunk_text (str): The text chunk.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use.\n",
    "        model (transformers.PreTrainedModel): The BERT model.\n",
    "        device (torch.device): The device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The embedding vector for the chunk.\n",
    "    \"\"\"\n",
    "    if not chunk_text:\n",
    "        # Return a zero vector if chunk_text is empty\n",
    "        return torch.zeros(model.config.hidden_size).to(device)\n",
    "    \n",
    "    # Tokenize the chunk with padding and truncation\n",
    "    tokens = tokenizer(\n",
    "        chunk_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=CHUNK_SIZE,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the specified device\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        # Use the CLS token embedding (first token)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: [1, hidden_size]\n",
    "    \n",
    "    return cls_embedding.squeeze()  # Shape: [hidden_size]\n",
    "\n",
    "def get_document_embedding(text, tokenizer, model, device, max_length=CHUNK_SIZE, max_total_tokens=MAX_TOTAL_TOKENS):\n",
    "    \"\"\"\n",
    "    Generates a single embedding for the entire document by averaging embeddings of its chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input document as a string.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use.\n",
    "        model (transformers.PreTrainedModel): The BERT model.\n",
    "        device (torch.device): The device to perform computations on.\n",
    "        max_length (int): Maximum number of tokens per chunk.\n",
    "        max_total_tokens (int): Maximum total tokens per document.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: The final document embedding as a list of floats.\n",
    "    \"\"\"\n",
    "    # Split the text into chunks\n",
    "    chunks = split_text_into_chunks(text, tokenizer, max_length=max_length, max_total_tokens=max_total_tokens)\n",
    "    \n",
    "    if not chunks:\n",
    "        # If the document is empty or cannot be tokenized, return a zero vector\n",
    "        return [0.0] * model.config.hidden_size\n",
    "    \n",
    "    # Initialize a tensor to accumulate embeddings\n",
    "    embedding_sum = torch.zeros(model.config.hidden_size).to(device)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Get embedding for the chunk\n",
    "        chunk_embedding = get_bert_embedding_chunk(chunk, tokenizer, model, device)\n",
    "        embedding_sum += chunk_embedding\n",
    "    \n",
    "    # Calculate the average embedding\n",
    "    final_embedding = embedding_sum / len(chunks)\n",
    "    \n",
    "    return final_embedding.cpu().tolist()  # Move to CPU and convert to list\n",
    "\n",
    "def generate_embeddings(df, text_column, embedding_column, tokenizer, model, device, max_length=CHUNK_SIZE, max_total_tokens=MAX_TOTAL_TOKENS):\n",
    "    \"\"\"\n",
    "    Applies the embedding generation process to each document in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing documents.\n",
    "        text_column (str): The name of the column containing text documents.\n",
    "        embedding_column (str): The name of the column to store embeddings.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use.\n",
    "        model (transformers.PreTrainedModel): The BERT model.\n",
    "        device (torch.device): The device to perform computations on.\n",
    "        max_length (int): Maximum number of tokens per chunk.\n",
    "        max_total_tokens (int): Maximum total tokens per document.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize the embedding column with empty lists\n",
    "    df[embedding_column] = None\n",
    "    \n",
    "    # Iterate over the DataFrame with a progress bar\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Generating BERT Embeddings\"):\n",
    "        text = row[text_column]\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            # Handle missing or non-string entries\n",
    "            df.at[idx, embedding_column] = None\n",
    "            continue\n",
    "        \n",
    "        # Generate the document embedding\n",
    "        embedding = get_document_embedding(text, tokenizer, model, device, max_length=max_length, max_total_tokens=max_total_tokens)\n",
    "        df.at[idx, embedding_column] = embedding\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ====================\n",
    "# 5.2. Generate Embeddings\n",
    "# ====================\n",
    "\n",
    "df_compinfo = generate_embeddings(\n",
    "    df=df_compinfo,\n",
    "    text_column=\"section_1\",\n",
    "    embedding_column=\"BERT-embedding\",\n",
    "    tokenizer=bert_tokenizer,\n",
    "    model=bert_model,\n",
    "    device=device,\n",
    "    max_length=CHUNK_SIZE,\n",
    "    max_total_tokens=MAX_TOTAL_TOKENS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0536a728-5632-467d-8512-2f2e15a4cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df_compinfo.to_pickle('BERT-embedded(1536).pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd637c52-e888-415b-8227-f319833f3bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f78909-35a3-4684-881d-81e1f219839a",
   "metadata": {},
   "source": [
    "# SBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf02d01d-391c-4513-aeba-6084c78b51fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Starting SBERT Embedding Generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2558 > 512). Running this sequence through the model will result in indexing errors\n",
      "Generating SBERT Embeddings: 100%|██████████| 27722/27722 [20:54<00:00, 22.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT Embedding Generation Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ====================\n",
    "# 1. Configuration\n",
    "# ====================\n",
    "\n",
    "# Define constants\n",
    "MAX_TOTAL_TOKENS = 1536       # Maximum total tokens per document\n",
    "CHUNK_SIZE = 512              # Maximum tokens per chunk\n",
    "BERT_MODEL_NAME = \"bert-base-uncased\"      # Pre-trained BERT model\n",
    "SBERT_MODEL_NAME = \"all-MiniLM-L6-v2\"      # Pre-trained SBERT model\n",
    "\n",
    "# ====================\n",
    "# 2. Setup Device\n",
    "# ====================\n",
    "\n",
    "# Utilize GPU if available for faster computations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ====================\n",
    "# 3. Load Tokenizer and Models\n",
    "# ====================\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)\n",
    "bert_model.to(device)\n",
    "bert_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Load SBERT model\n",
    "sbert_model = SentenceTransformer(SBERT_MODEL_NAME)\n",
    "sbert_model.to(device)\n",
    "\n",
    "# ====================\n",
    "# 4. Define Helper Functions\n",
    "# ====================\n",
    "\n",
    "def split_text_into_chunks(text, tokenizer, max_length=CHUNK_SIZE, max_total_tokens=MAX_TOTAL_TOKENS):\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks of tokens with a maximum length, ensuring total tokens do not exceed max_total_tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input document as a string.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use.\n",
    "        max_length (int): Maximum number of tokens per chunk.\n",
    "        max_total_tokens (int): Maximum total tokens per document.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Tokenize the entire text without adding special tokens\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    # Limit to the first max_total_tokens\n",
    "    tokens = tokens[:max_total_tokens]\n",
    "    \n",
    "    # Split tokens into chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_length):\n",
    "        chunk_tokens = tokens[i:i + max_length]\n",
    "        # Decode tokens back to string\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_bert_embedding_chunk(chunk_text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Generates the embedding for a single text chunk using BERT.\n",
    "\n",
    "    Args:\n",
    "        chunk_text (str): The text chunk.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use.\n",
    "        model (transformers.PreTrainedModel): The BERT model.\n",
    "        device (torch.device): The device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The embedding vector for the chunk.\n",
    "    \"\"\"\n",
    "    if not chunk_text:\n",
    "        # Return a zero vector if chunk_text is empty\n",
    "        return torch.zeros(model.config.hidden_size).to(device)\n",
    "    \n",
    "    # Tokenize the chunk with padding and truncation\n",
    "    tokens = tokenizer(\n",
    "        chunk_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=CHUNK_SIZE,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the specified device\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        # Use the CLS token embedding (first token)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: [1, hidden_size]\n",
    "    \n",
    "    return cls_embedding.squeeze()  # Shape: [hidden_size]\n",
    "\n",
    "def get_document_embedding(text, tokenizer, bert_model, device, max_length=CHUNK_SIZE, max_total_tokens=MAX_TOTAL_TOKENS):\n",
    "    \"\"\"\n",
    "    Generates a single embedding for the entire document by averaging embeddings of its BERT chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input document as a string.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use.\n",
    "        bert_model (transformers.PreTrainedModel): The BERT model.\n",
    "        device (torch.device): The device to perform computations on.\n",
    "        max_length (int): Maximum number of tokens per chunk.\n",
    "        max_total_tokens (int): Maximum total tokens per document.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: The final document embedding as a list of floats.\n",
    "    \"\"\"\n",
    "    # Split the text into chunks\n",
    "    chunks = split_text_into_chunks(text, tokenizer, max_length=max_length, max_total_tokens=max_total_tokens)\n",
    "    \n",
    "    if not chunks:\n",
    "        # If the document is empty or cannot be tokenized, return a zero vector\n",
    "        return [0.0] * bert_model.config.hidden_size\n",
    "    \n",
    "    # Initialize a tensor to accumulate embeddings\n",
    "    embedding_sum = torch.zeros(bert_model.config.hidden_size).to(device)\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Get embedding for the chunk\n",
    "        chunk_embedding = get_bert_embedding_chunk(chunk, tokenizer, bert_model, device)\n",
    "        embedding_sum += chunk_embedding\n",
    "    \n",
    "    # Calculate the average embedding\n",
    "    final_embedding = embedding_sum / len(chunks)\n",
    "    \n",
    "    return final_embedding.cpu().tolist()  # Move to CPU and convert to list\n",
    "\n",
    "def get_sbert_embedding(text, model):\n",
    "    \"\"\"\n",
    "    Generates the SBERT embedding for a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input document as a string.\n",
    "        model (SentenceTransformer): The SBERT model.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: The SBERT embedding as a list of floats.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or pd.isna(text):\n",
    "        return None\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = split_text_into_chunks(text, bert_tokenizer, max_length=CHUNK_SIZE, max_total_tokens=MAX_TOTAL_TOKENS)\n",
    "    \n",
    "    if not chunks:\n",
    "        return [0.0] * model.get_sentence_embedding_dimension()\n",
    "    \n",
    "    # Generate embeddings for all chunks\n",
    "    embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculate the average embedding\n",
    "    final_embedding = torch.mean(embeddings, dim=0)\n",
    "    \n",
    "    return final_embedding.cpu().tolist()\n",
    "\n",
    "def generate_bert_embeddings(df, text_column, embedding_column, tokenizer, model, device, max_length=CHUNK_SIZE, max_total_tokens=MAX_TOTAL_TOKENS):\n",
    "    \"\"\"\n",
    "    Applies the BERT embedding generation process to each document in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing documents.\n",
    "        text_column (str): The name of the column containing text documents.\n",
    "        embedding_column (str): The name of the column to store BERT embeddings.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to use.\n",
    "        model (transformers.PreTrainedModel): The BERT model.\n",
    "        device (torch.device): The device to perform computations on.\n",
    "        max_length (int): Maximum number of tokens per chunk.\n",
    "        max_total_tokens (int): Maximum total tokens per document.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added BERT embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize the embedding column with empty lists\n",
    "    df[embedding_column] = None\n",
    "    \n",
    "    # Iterate over the DataFrame with a progress bar\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Generating BERT Embeddings\"):\n",
    "        text = row[text_column]\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            # Handle missing or non-string entries\n",
    "            df.at[idx, embedding_column] = None\n",
    "            continue\n",
    "        \n",
    "        # Generate the document embedding\n",
    "        embedding = get_document_embedding(text, tokenizer, model, device, max_length=max_length, max_total_tokens=max_total_tokens)\n",
    "        df.at[idx, embedding_column] = embedding\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_sbert_embeddings(df, text_column, embedding_column, model, max_length=CHUNK_SIZE, max_total_tokens=MAX_TOTAL_TOKENS):\n",
    "    \"\"\"\n",
    "    Applies the SBERT embedding generation process to each document in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing documents.\n",
    "        text_column (str): The name of the column containing text documents.\n",
    "        embedding_column (str): The name of the column to store SBERT embeddings.\n",
    "        model (SentenceTransformer): The SBERT model.\n",
    "        max_length (int): Maximum number of tokens per chunk.\n",
    "        max_total_tokens (int): Maximum total tokens per document.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added SBERT embeddings.\n",
    "    \"\"\"\n",
    "    # Initialize the embedding column with empty lists\n",
    "    df[embedding_column] = None\n",
    "    \n",
    "    # Iterate over the DataFrame with a progress bar\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Generating SBERT Embeddings\"):\n",
    "        text = row[text_column]\n",
    "        if pd.isna(text) or not isinstance(text, str):\n",
    "            # Handle missing or non-string entries\n",
    "            df.at[idx, embedding_column] = None\n",
    "            continue\n",
    "        \n",
    "        # Generate the document embedding\n",
    "        embedding = get_sbert_embedding(text, model)\n",
    "        df.at[idx, embedding_column] = embedding\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# ====================\n",
    "# 5.3. Generate SBERT Embeddings\n",
    "# ====================\n",
    "\n",
    "print(\"\\nStarting SBERT Embedding Generation...\")\n",
    "df_compinfo = generate_sbert_embeddings(\n",
    "    df=df_compinfo,\n",
    "    text_column=\"section_1\",\n",
    "    embedding_column=\"SBERT-embedding\",\n",
    "    model=sbert_model,\n",
    "    max_length=CHUNK_SIZE,\n",
    "    max_total_tokens=MAX_TOTAL_TOKENS\n",
    ")\n",
    "print(\"SBERT Embedding Generation Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "392fa90d-3061-4ddc-83d6-72bdb8ab4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df_compinfo.to_pickle('SBERT_embedded(1536).pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb067026-a6ed-4350-b0f5-a7fa7c4f1763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_compinfo[\"SBERT-embedding\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148b1db-3080-4f83-ac01-6c94fd1a855a",
   "metadata": {},
   "source": [
    "# Get PALM-GECKO (below code ran on google cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3dbb83-062d-43e6-8e2c-ff4a25b0b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "\n",
    "# Function to get PALM-GECKO embeddings\n",
    "def get_gecko(text):\n",
    "  tasks = \"DEFAULT\"\n",
    "  inputs = [TextEmbeddingInput(text, tasks)]\n",
    "\n",
    "  embeddings = model.get_embeddings(inputs)\n",
    "  return [embedding.values for embedding in embeddings][0] # should be 1-D list\n",
    "\n",
    "# # Apply the embedding function to the DataFrame\n",
    "tqdm.pandas(desc=\"Generating PALM-gecko Embeddings\")\n",
    "df_compinfo[\"PALMGECKO-embedding\"] = df_compinfo[\"section_1\"].progress_apply(get_gecko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87f15a-3551-4222-86bb-c65b1831eb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
